{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces\n",
    "from scipy import ndimage\n",
    "\n",
    "class WoodCuttingEnv(gym.Env):\n",
    "    def __init__(self, big_platform_size=(100, 100), max_platforms=1000):\n",
    "        super(WoodCuttingEnv, self).__init__()\n",
    "        \n",
    "        # Initialize with the size of big wood platforms\n",
    "        self.big_platform_width, self.big_platform_height = big_platform_size\n",
    "        self.max_platforms = max_platforms\n",
    "        \n",
    "        # State: Representation of the current platform's state\n",
    "        # We'll represent it as a binary grid\n",
    "        self.grid_size = 100  # We'll use a 100x100 grid for simplicity\n",
    "        # Track piece types in each position (0 = empty, 1+ = piece type index + 1)\n",
    "        self.platforms = [np.zeros((self.grid_size, self.grid_size), dtype=np.int8)]\n",
    "        self.piece_types = [np.zeros((self.grid_size, self.grid_size), dtype=np.int8)]\n",
    "\n",
    "\n",
    "        # State space: Current platform's grid state + remaining order pieces + current platform index\n",
    "        grid_space = spaces.Box(low=0, high=1, shape=(self.grid_size, self.grid_size), dtype=np.int8)\n",
    "        \n",
    "        # Max number of different piece types in an order\n",
    "        self.max_order_types = 10\n",
    "        # Order space: (width, height, quantity) for each order type\n",
    "        order_space = spaces.Box(\n",
    "            low=np.array([[1, 1, 0]] * self.max_order_types),\n",
    "            high=np.array([[self.big_platform_width, self.big_platform_height, 100]] * self.max_order_types),\n",
    "            dtype=np.int32\n",
    "        )\n",
    "        \n",
    "        # Current platform index\n",
    "        platform_index_space = spaces.Discrete(self.max_platforms + 1)  # +1 for \"no platforms left\"\n",
    "        \n",
    "        # Combine spaces\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'grid': grid_space,\n",
    "            'order': order_space,\n",
    "            'platform_index': platform_index_space\n",
    "        })\n",
    "        \n",
    "        # Action space: (x, y, piece_type, rotation)\n",
    "        # x, y: position to place the piece\n",
    "        # piece_type: which piece type from the order to use\n",
    "        # rotation: 0 or 1 (0° or 90°)\n",
    "        self.action_space = spaces.MultiDiscrete([\n",
    "            self.grid_size,  # x\n",
    "            self.grid_size,  # y\n",
    "            self.max_order_types,  # piece_type\n",
    "            2  # rotation (0 or 1)\n",
    "        ])\n",
    "        \n",
    "        # Initialize state\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self, order=None):\n",
    "        \"\"\"Reset environment with a new or provided order.\"\"\"\n",
    "        # Initialize the first platform grid (0 = empty, 1 = filled)\n",
    "        self.platforms = [np.zeros((self.grid_size, self.grid_size), dtype=np.int8)]\n",
    "        self.piece_types = [np.zeros((self.grid_size, self.grid_size), dtype=np.int8)]\n",
    "        self.current_platform_idx = 0\n",
    "        \n",
    "        # Generate a random order if none provided\n",
    "        if order is None:\n",
    "            self.order = self._generate_random_order()\n",
    "        else:\n",
    "            self.order = order.copy()\n",
    "        \n",
    "        return self._get_observation()\n",
    "    \n",
    "    def _generate_random_order(self):\n",
    "        \"\"\"Generate a random cutting order.\"\"\"\n",
    "        num_types = np.random.randint(1, self.max_order_types + 1)\n",
    "        order = []\n",
    "        \n",
    "        for _ in range(num_types):\n",
    "            width = np.random.randint(5, min(self.big_platform_width // 2, 30) + 1)\n",
    "            height = np.random.randint(5, min(self.big_platform_height // 2, 30) + 1)\n",
    "            quantity = np.random.randint(1, 20)\n",
    "            order.append([width, height, quantity])\n",
    "        \n",
    "        # Pad the order to max_order_types\n",
    "        while len(order) < self.max_order_types:\n",
    "            order.append([0, 0, 0])\n",
    "            \n",
    "        return np.array(order)\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        \"\"\"Return the current observation.\"\"\"\n",
    "        return {\n",
    "            'grid': self.platforms[self.current_platform_idx],\n",
    "            'order': self.order,\n",
    "            'platform_index': self.current_platform_idx\n",
    "        }\n",
    "    \n",
    "    def _is_valid_placement(self, x, y, piece_width, piece_height, platform_idx=None):\n",
    "        \"\"\"Check if a piece can be placed at (x, y) with given dimensions.\"\"\"\n",
    "        if platform_idx is None:\n",
    "            platform_idx = self.current_platform_idx\n",
    "            \n",
    "        if x + piece_width > self.grid_size or y + piece_height > self.grid_size:\n",
    "            return False\n",
    "        \n",
    "        # Check if the area is empty\n",
    "        if np.any(self.platforms[platform_idx][y:y+piece_height, x:x+piece_width] == 1):\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _can_fit_anywhere(self, piece_width, piece_height, platform_idx=None):\n",
    "        \"\"\"Check if a piece can fit anywhere on the specified platform.\"\"\"\n",
    "        if platform_idx is None:\n",
    "            platform_idx = self.current_platform_idx\n",
    "            \n",
    "        for y in range(self.grid_size - piece_height + 1):\n",
    "            for x in range(self.grid_size - piece_width + 1):\n",
    "                if self._is_valid_placement(x, y, piece_width, piece_height, platform_idx):\n",
    "                    return True, (x, y)\n",
    "        return False, None\n",
    "    \n",
    "    def _place_piece(self, x, y, piece_width, piece_height, piece_type, platform_idx=None):\n",
    "        \"\"\"Place a piece at (x, y) with given dimensions.\"\"\"\n",
    "        if platform_idx is None:\n",
    "            platform_idx = self.current_platform_idx\n",
    "            \n",
    "        self.platforms[platform_idx][y:y+piece_height, x:x+piece_width] = 1\n",
    "        self.piece_types[platform_idx][y:y+piece_height, x:x+piece_width] = piece_type + 1  # +1 to avoid 0\n",
    "    \n",
    "    def calculate_metrics(self):\n",
    "        \"\"\"\n",
    "        Calculate various efficiency metrics for the current cutting solution.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Dictionary containing the following metrics:\n",
    "                - waste_rate: Total unused area / Total area of all cut stock\n",
    "                - fitness: Total area of all cut items / Total area of all cut stock\n",
    "                - platforms_used: Number of platforms used\n",
    "                - filled_area: Total area filled with cut items\n",
    "                - total_stock_area: Total area of all cut stock\n",
    "        \"\"\"\n",
    "        # Count how many platforms were actually used\n",
    "        used_platforms = 0\n",
    "        filled_area = 0\n",
    "        \n",
    "        for platform in self.platforms:\n",
    "            if np.any(platform == 1):\n",
    "                used_platforms += 1\n",
    "                filled_area += np.sum(platform)\n",
    "        \n",
    "        # Calculate total stock area (area of all used platforms)\n",
    "        total_stock_area = self.big_platform_width * self.big_platform_height * used_platforms\n",
    "        \n",
    "        # Calculate waste rate\n",
    "        waste_area = total_stock_area - filled_area\n",
    "        waste_rate = waste_area / total_stock_area if total_stock_area > 0 else 0\n",
    "        \n",
    "        # Calculate fitness\n",
    "        fitness = filled_area / total_stock_area if total_stock_area > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'waste_rate': waste_rate,\n",
    "            'fitness': fitness,\n",
    "            'platforms_used': used_platforms,\n",
    "            'filled_area': filled_area,\n",
    "            'total_stock_area': total_stock_area,\n",
    "            'waste_area': waste_area\n",
    "        }\n",
    "    \n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take a step in the environment.\n",
    "        \n",
    "        Args:\n",
    "            action: [x, y, piece_type, rotation]\n",
    "        \n",
    "        Returns:\n",
    "            observation, reward, done, info\n",
    "        \"\"\"\n",
    "        x, y, piece_type, rotation = action\n",
    "        \n",
    "        # Check if piece_type is valid\n",
    "        if piece_type >= len(self.order) or self.order[piece_type][2] <= 0:\n",
    "            # Invalid piece type or no more pieces of this type\n",
    "            return self._get_observation(), -10, False, {'message': 'Invalid piece type'}\n",
    "        \n",
    "        # Get piece dimensions\n",
    "        width, height, quantity = self.order[piece_type]\n",
    "        \n",
    "        # Apply rotation if needed\n",
    "        if rotation == 1:\n",
    "            width, height = height, width\n",
    "        \n",
    "        # Check if placement is valid at the chosen position on the current platform\n",
    "        if not self._is_valid_placement(x, y, width, height):\n",
    "            # Try to find placement on any existing platform\n",
    "            placed = False\n",
    "            best_position = None\n",
    "            best_platform = None\n",
    "            \n",
    "            # Check all existing platforms\n",
    "            for platform_idx in range(len(self.platforms)):\n",
    "                can_fit, new_pos = self._can_fit_anywhere(width, height, platform_idx)\n",
    "                if can_fit:\n",
    "                    if best_position is None or platform_idx <= best_platform:\n",
    "                        best_position = new_pos\n",
    "                        best_platform = platform_idx\n",
    "                        placed = True\n",
    "            \n",
    "            if placed:\n",
    "                # Place at the best position found\n",
    "                x, y = best_position\n",
    "                self._place_piece(x, y, width, height, piece_type, best_platform)\n",
    "                self.order[piece_type][2] -= 1\n",
    "                # Set the current platform to where we placed the piece\n",
    "                self.current_platform_idx = best_platform\n",
    "                reward = width * height - 5  # Small penalty for repositioning\n",
    "            else:\n",
    "                # Try creating a new platform if allowed\n",
    "                if self.current_platform_idx + 1 < self.max_platforms:\n",
    "                    new_platform_idx = len(self.platforms)\n",
    "                    self.platforms.append(np.zeros((self.grid_size, self.grid_size), dtype=np.int8))\n",
    "                    self.piece_types.append(np.zeros((self.grid_size, self.grid_size), dtype=np.int8))\n",
    "                    \n",
    "                    # Place at the beginning of the new platform\n",
    "                    if self._is_valid_placement(0, 0, width, height, new_platform_idx):\n",
    "                        self._place_piece(0, 0, width, height, piece_type, new_platform_idx)\n",
    "                        self.order[piece_type][2] -= 1\n",
    "                        self.current_platform_idx = new_platform_idx\n",
    "                        reward = width * height - 50  # Reward for successful placement minus penalty for new platform\n",
    "                    else:\n",
    "                        reward = -20  # Penalty for invalid placement even on new platform\n",
    "                else:\n",
    "                    reward = -30  # No more platforms available\n",
    "                    return self._get_observation(), reward, True, {'message': 'No more platforms'}\n",
    "        else:\n",
    "            # Place the piece at the original position on current platform\n",
    "            self._place_piece(x, y, width, height, piece_type)\n",
    "            self.order[piece_type][2] -= 1\n",
    "            reward = width * height  # Reward proportional to the piece area\n",
    "        \n",
    "        # Check if all pieces have been placed\n",
    "        done = np.all(self.order[:, 2] == 0)\n",
    "        \n",
    "        if done:\n",
    "            # Calculate metrics\n",
    "            metrics = self.calculate_metrics()\n",
    "            \n",
    "            # Add final reward based on fitness\n",
    "            reward += metrics['fitness'] * 1000\n",
    "            \n",
    "            return self._get_observation(), reward, done, {\n",
    "                'message': 'All pieces placed',\n",
    "                'waste_rate': metrics['waste_rate'],\n",
    "                'fitness': metrics['fitness'],\n",
    "                'platforms_used': metrics['platforms_used'],\n",
    "                'filled_area': metrics['filled_area'],\n",
    "                'total_stock_area': metrics['total_stock_area']\n",
    "            }\n",
    "        \n",
    "        return self._get_observation(), reward, done, {}\n",
    "    \n",
    "    def render(self):\n",
    "        # \"\"\"Render the current state of the environment.\"\"\"\n",
    "        \"\"\"Render the current state of the environment with different colors for each piece type.\"\"\"\n",
    "        piece_colors = ['white', 'red', 'blue', 'green', 'purple', 'orange', 'yellow', 'black', 'gray', 'pink', 'brown']\n",
    "        fig, axs = plt.subplots(1, len(self.platforms), figsize=(5*len(self.platforms), 5))\n",
    "\n",
    "        if len(self.platforms) == 1:\n",
    "            axs = [axs]\n",
    "        \n",
    "        for i, platform in enumerate(self.platforms):\n",
    "            ax = axs[i]\n",
    "            \n",
    "            # Create a colored image\n",
    "            colored_image = np.zeros((self.grid_size, self.grid_size, 3))\n",
    "            \n",
    "            # Fill with colors based on piece types\n",
    "            for y in range(self.grid_size):\n",
    "                for x in range(self.grid_size):\n",
    "                    piece_type = self.piece_types[i][y, x]\n",
    "                    if piece_type > 0:\n",
    "                        # Convert color name to RGB\n",
    "                        color_name = piece_colors[piece_type]\n",
    "                        color_rgb = np.array(plt.matplotlib.colors.to_rgb(color_name))\n",
    "                        colored_image[y, x] = color_rgb\n",
    "            \n",
    "            ax.imshow(colored_image)\n",
    "            ax.set_title(f'Platform {i+1}')\n",
    "            ax.set_xlim(0, self.grid_size)\n",
    "            ax.set_ylim(0, self.grid_size)\n",
    "            ax.invert_yaxis()  # Invert y-axis to match grid coordinates\n",
    "            \n",
    "            ax.grid(True, color='black', linewidth=0.5, alpha=0.3)\n",
    "            \n",
    "            # Add piece outlines\n",
    "            for type_id in range(1, self.max_order_types + 1):\n",
    "                piece_mask = (self.piece_types[i] == type_id)\n",
    "                if not np.any(piece_mask):\n",
    "                    continue\n",
    "                    \n",
    "        # Display remaining order with colors\n",
    "        order_text = []\n",
    "        for idx, (w, h, q) in enumerate(self.order):\n",
    "            if q > 0:\n",
    "                color = piece_colors[idx + 1]\n",
    "                order_text.append(f\"<span style='color:{color}'>{w}x{h} (qty: {q})</span>\")\n",
    "        \n",
    "        if order_text:\n",
    "            from matplotlib.text import Text\n",
    "            plt.figtext(0.5, 0.01, f\"Remaining pieces: {', '.join(order_text)}\", \n",
    "                    ha=\"center\", fontsize=9, bbox={\"facecolor\":\"white\", \"alpha\":0.8})\n",
    "        else:\n",
    "            plt.figtext(0.5, 0.01, \"All pieces placed!\", ha=\"center\", fontsize=9, \n",
    "                    bbox={\"facecolor\":\"green\", \"alpha\":0.5})\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(bottom=0.15)\n",
    "        plt.show()\n",
    "    def close(self):\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import random\n",
    "\n",
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, action_dims):\n",
    "        super(ActorCriticNetwork, self).__init__()\n",
    "        \n",
    "        # CNN for processing the grid\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=2)\n",
    "        \n",
    "        # Calculate the size after convolutions\n",
    "        def conv2d_size_out(size, kernel_size=3, stride=2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "        conv_width = conv2d_size_out(conv2d_size_out(conv2d_size_out(input_shape[1], 5, 2)))\n",
    "        conv_height = conv2d_size_out(conv2d_size_out(conv2d_size_out(input_shape[0], 5, 2)))\n",
    "        linear_input_size = conv_width * conv_height * 64\n",
    "        \n",
    "        # FC for processing the order information\n",
    "        self.fc_order = nn.Linear(30, 128)  # 10 order types x 3 features (width, height, quantity)\n",
    "        \n",
    "        # FC for processing platform index\n",
    "        self.fc_platform = nn.Linear(1, 32)\n",
    "        \n",
    "        # Combine and process\n",
    "        self.fc_combine = nn.Linear(linear_input_size + 128 + 32, 512)\n",
    "        \n",
    "        # Actor head (policy)\n",
    "        self.actor_x = nn.Linear(512, action_dims[0])  # x position\n",
    "        self.actor_y = nn.Linear(512, action_dims[1])  # y position\n",
    "        self.actor_piece = nn.Linear(512, action_dims[2])  # piece type\n",
    "        self.actor_rotation = nn.Linear(512, action_dims[3])  # rotation\n",
    "        \n",
    "        # Critic head (value function)\n",
    "        self.critic = nn.Linear(512, 1)\n",
    "        \n",
    "    def forward(self, grid, order, platform_idx):\n",
    "        # Process grid\n",
    "        grid = grid.unsqueeze(1)  # Add channel dimension\n",
    "        grid = F.relu(self.conv1(grid))\n",
    "        grid = F.relu(self.conv2(grid))\n",
    "        grid = F.relu(self.conv3(grid))\n",
    "        grid = grid.view(grid.size(0), -1)  # Flatten\n",
    "        \n",
    "        # Process order\n",
    "        order = order.view(order.size(0), -1)  # Flatten\n",
    "        order = F.relu(self.fc_order(order))\n",
    "        \n",
    "        # Process platform index\n",
    "        platform_idx = platform_idx.float().view(-1, 1)\n",
    "        platform = F.relu(self.fc_platform(platform_idx))\n",
    "        \n",
    "        # Combine\n",
    "        combined = torch.cat((grid, order, platform), dim=1)\n",
    "        features = F.relu(self.fc_combine(combined))\n",
    "        \n",
    "        # Actor outputs (policy distributions)\n",
    "        x_logits = self.actor_x(features)\n",
    "        y_logits = self.actor_y(features)\n",
    "        piece_logits = self.actor_piece(features)\n",
    "        rotation_logits = self.actor_rotation(features)\n",
    "        \n",
    "        # Critic output (value function)\n",
    "        value = self.critic(features)\n",
    "        \n",
    "        return x_logits, y_logits, piece_logits, rotation_logits, value\n",
    "\n",
    "class PPOMemory:\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "    \n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "        \n",
    "        return batches\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_shape, action_space, device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                 lr=0.0003, gamma=0.99, gae_lambda=0.95, policy_clip=0.2, \n",
    "                 batch_size=64, n_epochs=10, entropy_coefficient=0.01):\n",
    "        self.state_shape = state_shape\n",
    "        self.action_space = action_space\n",
    "        self.device = device\n",
    "        \n",
    "        # PPO hyperparameters\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.entropy_coefficient = entropy_coefficient\n",
    "        \n",
    "        # Create actor-critic network\n",
    "        self.actor_critic = ActorCriticNetwork(\n",
    "            input_shape=(state_shape['grid'][0], state_shape['grid'][1]),\n",
    "            action_dims=(action_space[0], action_space[1], action_space[2], action_space[3])\n",
    "        ).to(device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\n",
    "        self.memory = PPOMemory(batch_size)\n",
    "        \n",
    "        # For handling actions\n",
    "        self.grid_size = state_shape['grid'][0]\n",
    "        self.max_order_types = state_shape['order'][0]\n",
    "        self.rotations = action_space[3]\n",
    "        \n",
    "    def choose_action(self, state, training=True):\n",
    "        # Convert state to tensors\n",
    "        grid = torch.FloatTensor(state['grid']).unsqueeze(0).to(self.device)\n",
    "        order = torch.FloatTensor(state['order']).unsqueeze(0).to(self.device)\n",
    "        platform_idx = torch.LongTensor([state['platform_index']]).to(self.device)\n",
    "        \n",
    "        # Get action distributions and value from actor-critic\n",
    "        with torch.no_grad():\n",
    "            x_logits, y_logits, piece_logits, rotation_logits, value = self.actor_critic(\n",
    "                grid, order, platform_idx\n",
    "            )\n",
    "        \n",
    "        # Create distributions for each action component\n",
    "        x_dist = Categorical(F.softmax(x_logits, dim=1))\n",
    "        y_dist = Categorical(F.softmax(y_logits, dim=1))\n",
    "        \n",
    "        # Mask invalid piece types (pieces with quantity 0)\n",
    "        piece_mask = torch.ones_like(piece_logits) * float('-inf')\n",
    "        for i, (_, _, qty) in enumerate(state['order']):\n",
    "            if qty > 0:\n",
    "                piece_mask[0, i] = 0  # Unmask valid piece types\n",
    "        \n",
    "        masked_piece_logits = piece_logits + piece_mask\n",
    "        piece_dist = Categorical(F.softmax(masked_piece_logits, dim=1))\n",
    "        rotation_dist = Categorical(F.softmax(rotation_logits, dim=1))\n",
    "        \n",
    "        # Sample actions from distributions (or take most likely action during evaluation)\n",
    "        if training:\n",
    "            x = x_dist.sample()\n",
    "            y = y_dist.sample()\n",
    "            piece_type = piece_dist.sample()\n",
    "            rotation = rotation_dist.sample()\n",
    "        else:\n",
    "            x = torch.argmax(x_dist.probs)\n",
    "            y = torch.argmax(y_dist.probs)\n",
    "            piece_type = torch.argmax(piece_dist.probs)\n",
    "            rotation = torch.argmax(rotation_dist.probs)\n",
    "        \n",
    "        # Calculate log probabilities\n",
    "        x_prob = x_dist.log_prob(x)\n",
    "        y_prob = y_dist.log_prob(y)\n",
    "        piece_prob = piece_dist.log_prob(piece_type)\n",
    "        rotation_prob = rotation_dist.log_prob(rotation)\n",
    "        \n",
    "        # Sum the log probs to get the total action log probability\n",
    "        action_log_prob = x_prob + y_prob + piece_prob + rotation_prob\n",
    "        \n",
    "        return [x.item(), y.item(), piece_type.item(), rotation.item()], action_log_prob.item(), value.item()\n",
    "    \n",
    "    def store_transition(self, state, action, action_log_prob, value, reward, done):\n",
    "        self.memory.store_memory(state, action, action_log_prob, value, reward, done)\n",
    "    \n",
    "    def learn(self):\n",
    "        for _ in range(self.n_epochs):\n",
    "            # Calculate advantages and returns\n",
    "            state_arr, action_arr, old_prob_arr, vals_arr, reward_arr, done_arr = self._process_memory()\n",
    "            \n",
    "            # Generate mini-batches\n",
    "            batches = self.memory.generate_batches()\n",
    "            \n",
    "            # Train on each batch\n",
    "            for batch in batches:\n",
    "                # Select batch components\n",
    "                grids = torch.FloatTensor(state_arr['grid'][batch]).to(self.device)\n",
    "                orders = torch.FloatTensor(state_arr['order'][batch]).to(self.device)\n",
    "                platform_idxs = torch.LongTensor(state_arr['platform_index'][batch]).to(self.device)\n",
    "                \n",
    "                actions = action_arr[batch]\n",
    "                old_probs = old_prob_arr[batch]\n",
    "                values = vals_arr[batch]\n",
    "                \n",
    "                # Calculate advantages and returns for the batch\n",
    "                advantages = self._calculate_advantages(\n",
    "                    reward_arr[batch], values, done_arr[batch]\n",
    "                )\n",
    "                returns = advantages + values\n",
    "                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "                \n",
    "                # Forward pass\n",
    "                x_logits, y_logits, piece_logits, rotation_logits, critic_value = self.actor_critic(\n",
    "                    grids, orders, platform_idxs\n",
    "                )\n",
    "                \n",
    "                # Extract action components\n",
    "                x = actions[:, 0]\n",
    "                y = actions[:, 1]\n",
    "                piece_type = actions[:, 2]\n",
    "                rotation = actions[:, 3]\n",
    "                \n",
    "                # Create distributions\n",
    "                x_dist = Categorical(F.softmax(x_logits, dim=1))\n",
    "                y_dist = Categorical(F.softmax(y_logits, dim=1))\n",
    "                piece_dist = Categorical(F.softmax(piece_logits, dim=1))\n",
    "                rotation_dist = Categorical(F.softmax(rotation_logits, dim=1))\n",
    "                \n",
    "                # Calculate new log probabilities\n",
    "                x_new_probs = x_dist.log_prob(torch.LongTensor(x).to(self.device))\n",
    "                y_new_probs = y_dist.log_prob(torch.LongTensor(y).to(self.device))\n",
    "                piece_new_probs = piece_dist.log_prob(torch.LongTensor(piece_type).to(self.device))\n",
    "                rotation_new_probs = rotation_dist.log_prob(torch.LongTensor(rotation).to(self.device))\n",
    "                \n",
    "                # Combine log probabilities\n",
    "                new_probs = x_new_probs + y_new_probs + piece_new_probs + rotation_new_probs\n",
    "                \n",
    "                # Calculate probability ratio\n",
    "                prob_ratio = torch.exp(new_probs - torch.FloatTensor(old_probs).to(self.device))\n",
    "                \n",
    "                # Calculate surrogate losses\n",
    "                weighted_probs = advantages.to(self.device) * prob_ratio\n",
    "                weighted_clipped_probs = advantages.to(self.device) * torch.clamp(\n",
    "                    prob_ratio, 1-self.policy_clip, 1+self.policy_clip\n",
    "                )\n",
    "                actor_loss = -torch.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "                \n",
    "                # Add entropy bonus for exploration\n",
    "                entropy = x_dist.entropy().mean() + y_dist.entropy().mean() + \\\n",
    "                        piece_dist.entropy().mean() + rotation_dist.entropy().mean()\n",
    "                \n",
    "                # Calculate critic loss\n",
    "                returns = returns.float().to(self.device)  # Ensure returns is float32\n",
    "                critic_loss = F.mse_loss(critic_value.squeeze(), returns)\n",
    "                \n",
    "                # Calculate total loss\n",
    "                total_loss = actor_loss + 0.5 * critic_loss - self.entropy_coefficient * entropy\n",
    "                \n",
    "                # Update network\n",
    "                self.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.optimizer.step()\n",
    "        \n",
    "        # Clear memory after learning\n",
    "        self.memory.clear_memory()\n",
    "\n",
    "    def _calculate_advantages(self, rewards, values, dones):\n",
    "        \"\"\"Calculate advantages using Generalized Advantage Estimation (GAE).\"\"\"\n",
    "        advantages = np.zeros_like(rewards)\n",
    "        gae = 0\n",
    "        \n",
    "        for t in reversed(range(len(rewards))):\n",
    "            if t == len(rewards) - 1:\n",
    "                next_value = 0\n",
    "            else:\n",
    "                next_value = values[t + 1]\n",
    "            \n",
    "            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae\n",
    "            advantages[t] = gae\n",
    "        \n",
    "        return torch.tensor(advantages, dtype=torch.float32)  # Explicitly use float32\n",
    "    \n",
    "    def _process_memory(self):\n",
    "        \"\"\"Process memory data into arrays.\"\"\"\n",
    "        # States need special handling due to being dictionaries\n",
    "        state_arr = {\n",
    "            'grid': [],\n",
    "            'order': [],\n",
    "            'platform_index': []\n",
    "        }\n",
    "        \n",
    "        for state in self.memory.states:\n",
    "            state_arr['grid'].append(state['grid'])\n",
    "            state_arr['order'].append(state['order'])\n",
    "            state_arr['platform_index'].append(state['platform_index'])\n",
    "        \n",
    "        # Convert states to numpy arrays\n",
    "        state_arr = {k: np.array(v) for k, v in state_arr.items()}\n",
    "        \n",
    "        # Convert other memory components to numpy arrays\n",
    "        action_arr = np.array(self.memory.actions)\n",
    "        old_prob_arr = np.array(self.memory.probs)\n",
    "        vals_arr = np.array(self.memory.vals)\n",
    "        reward_arr = np.array(self.memory.rewards)\n",
    "        done_arr = np.array(self.memory.dones)\n",
    "        \n",
    "        return state_arr, action_arr, old_prob_arr, vals_arr, reward_arr, done_arr\n",
    "    \n",
    "    def _calculate_advantages(self, rewards, values, dones):\n",
    "        \"\"\"Calculate advantages using Generalized Advantage Estimation (GAE).\"\"\"\n",
    "        advantages = np.zeros_like(rewards)\n",
    "        gae = 0\n",
    "        \n",
    "        for t in reversed(range(len(rewards))):\n",
    "            if t == len(rewards) - 1:\n",
    "                next_value = 0\n",
    "            else:\n",
    "                next_value = values[t + 1]\n",
    "            \n",
    "            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae\n",
    "            advantages[t] = gae\n",
    "        \n",
    "        return torch.FloatTensor(advantages)\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save({\n",
    "            'actor_critic': self.actor_critic.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict()\n",
    "        }, path)\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        self.actor_critic.load_state_dict(checkpoint['actor_critic'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "def custom_order_example():\n",
    "    \"\"\"Example of using a custom order.\"\"\"\n",
    "    # Create environment\n",
    "    env = WoodCuttingEnv()\n",
    "    \n",
    "    # Create and load agent\n",
    "    agent = PPOAgent(\n",
    "        state_shape={\n",
    "            'grid': env.observation_space['grid'].shape,\n",
    "            'order': env.observation_space['order'].shape,\n",
    "            'platform_index': (1,)\n",
    "        },\n",
    "        action_space=env.action_space.nvec\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        agent.load_model(r'models/ppo_wood_cutting_final.pth')\n",
    "        print(\"Loaded trained model\")\n",
    "    except:\n",
    "        print(\"No trained model found, using untrained agent\")\n",
    "    \n",
    "    multiple_orders = [\n",
    "    [[40, 50, 2], [20, 25, 4], [4, 25, 10], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_001\n",
    "    [[40, 50, 1], [20, 25, 6], [4, 25, 20], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_002\n",
    "    [[40, 50, 3], [20, 25, 6], [4, 25, 25], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_003\n",
    "    [[40, 50, 4], [20, 25, 8], [4, 25, 30], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_004\n",
    "    [[40, 50, 5], [20, 25, 10], [4, 25, 35], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_005\n",
    "    [[40, 50, 6], [20, 25, 12], [4, 25, 40], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_006\n",
    "    [[40, 50, 7], [20, 25, 14], [4, 25, 45], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_007\n",
    "    [[40, 50, 8], [20, 25, 16], [4, 25, 50], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_008\n",
    "    [[40, 50, 4], [20, 25, 8], [4, 25, 30], [50, 60, 2], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_009\n",
    "    [[40, 50, 5], [20, 25, 10], [4, 25, 35], [50, 60, 3], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_010\n",
    "    [[40, 50, 6], [20, 25, 12], [4, 25, 40], [50, 60, 4], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_011\n",
    "    [[40, 50, 7], [20, 25, 14], [4, 25, 45], [50, 60, 5], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_012\n",
    "    [[40, 50, 3], [20, 25, 6], [4, 25, 20], [30, 70, 2], [60, 80, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_013\n",
    "    [[40, 50, 4], [20, 25, 8], [4, 25, 25], [30, 70, 3], [60, 80, 2], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_014\n",
    "    [[40, 50, 5], [20, 25, 10], [4, 25, 30], [30, 70, 4], [60, 80, 3], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_015\n",
    "    [[40, 50, 6], [20, 25, 12], [4, 25, 35], [30, 70, 5], [60, 80, 4], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_016\n",
    "    [[40, 50, 5], [20, 25, 10], [4, 25, 40], [50, 60, 2], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_017\n",
    "    [[40, 50, 6], [20, 25, 12], [4, 25, 45], [30, 70, 3], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_018\n",
    "    [[40, 50, 7], [20, 25, 14], [4, 25, 50], [60, 80, 2], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_019\n",
    "    [[40, 50, 8], [20, 25, 16], [4, 25, 55], [50, 60, 3], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_020\n",
    "    [[40, 50, 9], [20, 25, 18], [4, 25, 60], [30, 70, 4], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_021\n",
    "    [[40, 50, 10], [20, 25, 20], [4, 25, 65], [60, 80, 3], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_022\n",
    "    [[40, 50, 11], [20, 25, 22], [4, 25, 70], [50, 60, 4], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_023\n",
    "    [[40, 50, 12], [20, 25, 24], [4, 25, 75], [30, 70, 5], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_024\n",
    "    [[40, 50, 13], [20, 25, 26], [4, 25, 80], [60, 80, 4], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_025\n",
    "    [[40, 50, 14], [20, 25, 28], [4, 25, 85], [50, 60, 5], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_026\n",
    "    [[40, 50, 15], [20, 25, 30], [4, 25, 90], [30, 70, 6], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_027\n",
    "    [[40, 50, 16], [20, 25, 32], [4, 25, 95], [60, 80, 5], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_028\n",
    "    [[40, 50, 17], [20, 25, 34], [4, 25, 100], [50, 60, 6], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_029\n",
    "    [[40, 50, 18], [20, 25, 36], [4, 25, 105], [30, 70, 7], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_030\n",
    "    [[45, 55, 20], [22, 27, 40], [4, 26, 110], [32, 72, 8], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_031\n",
    "    [[50, 60, 22], [24, 28, 45], [5, 27, 120], [35, 75, 9], [60, 80, 5], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_032\n",
    "    [[55, 65, 24], [26, 30, 50], [5, 28, 130], [38, 78, 10], [65, 85, 6], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_033\n",
    "    [[60, 70, 26], [28, 32, 55], [6, 30, 140], [40, 80, 11], [70, 90, 7], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_034\n",
    "    [[65, 75, 28], [30, 34, 60], [6, 32, 150], [42, 82, 12], [75, 95, 8], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_035\n",
    "    [[70, 80, 30], [32, 36, 65], [7, 34, 160], [45, 80, 13], [80, 80, 9], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_036\n",
    "    [[75, 80, 32], [34, 38, 70], [7, 36, 170], [48, 20, 14], [80, 60, 10], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_037\n",
    "    [[80, 80, 34], [36, 40, 75], [8, 38, 180], [50, 60, 15], [80, 40, 11], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_038\n",
    "    [[50, 20, 36], [38, 42, 80], [8, 40, 190], [52, 80, 16], [30, 30, 12], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_039\n",
    "    [[50, 10, 38], [40, 44, 85], [9, 42, 200], [55, 40, 17], [40, 50, 13], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_040\n",
    "    [[50, 80, 40], [42, 46, 90], [9, 44, 210], [58, 80, 18], [50, 20, 14], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_041\n",
    "    [[50, 60, 42], [44, 48, 95], [10, 46, 220], [60, 30, 19], [70, 30, 15], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_042\n",
    "    [[50, 30, 44], [46, 50, 100], [10, 48, 230], [62, 40, 20], [30, 60, 16], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_043\n",
    "    [[20, 80, 46], [48, 52, 105], [11, 50, 240], [65, 80, 21], [40, 30, 17], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_044\n",
    "    [[20, 70, 48], [50, 54, 110], [11, 52, 250], [68, 43, 22], [70, 10, 18], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_045\n",
    "    [[20, 40, 50], [52, 56, 115], [12, 54, 260], [70, 67, 23], [30, 50, 19], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_046\n",
    "    [[20, 80, 52], [54, 58, 120], [12, 56, 270], [72, 38, 24], [30, 30, 20], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_047\n",
    "    [[20, 80, 54], [56, 60, 125], [13, 58, 280], [75, 80, 25], [90, 30, 21], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_048\n",
    "    [[50, 80, 56], [58, 62, 130], [13, 60, 290], [78, 80, 26], [34, 20, 22], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],  # order_049\n",
    "    [[50, 80, 58], [60, 64, 135], [14, 62, 300], [25, 80, 27], [20, 80, 23], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]   # order_050\n",
    "]\n",
    "    evaluating_history_file_path = fr'result/evaluating_history_data.npz'\n",
    "    \n",
    "    # Remove the old history file\n",
    "    if os.path.exists(evaluating_history_file_path):\n",
    "        os.remove(evaluating_history_file_path)\n",
    "\n",
    "    waste_rate_history = []\n",
    "    fitness_history = []\n",
    "    platforms_used_history = []\n",
    "    runtime_history = []\n",
    "\n",
    "    for custom_order in multiple_orders:\n",
    "        start_time = time.perf_counter()\n",
    "        custom_order = np.array(custom_order)\n",
    "        \n",
    "        # Reset environment with custom order\n",
    "        state = env.reset(order=custom_order)\n",
    "        \n",
    "        # Run episode with custom order\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        max_steps = 1000000\n",
    "\n",
    "        # print(steps)\n",
    "        # env.render()\n",
    "        \n",
    "        print(\"Starting custom order optimization...\")\n",
    "        \n",
    "        while not done and steps < max_steps:\n",
    "            # Choose action\n",
    "            action, _, _  = agent.choose_action(state, training=False)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Update state and total reward\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            # print(\"step\",steps)\n",
    "            # env.render()\n",
    "            \n",
    "            if done:\n",
    "                end_time = time.perf_counter()\n",
    "                runtime = end_time - start_time\n",
    "                print(f\"Order completed in {steps} steps\")\n",
    "                print(f\"Total reward: {total_reward:.2f}\")\n",
    "                if 'waste_rate' in info:\n",
    "                    waste_rate_history.append(info['waste_rate'])\n",
    "                    fitness_history.append(info['fitness'])\n",
    "                    platforms_used_history.append(info['platforms_used'])\n",
    "                    runtime_history.append(runtime)\n",
    "                    print(f\"Waste Rate: {info['waste_rate']:.4f}\")\n",
    "                    print(f\"Fitness: {info['fitness']:.4f}\")\n",
    "                    print(f\"Platforms used: {info['platforms_used']}\")\n",
    "                    print(f\"Filled Area: {info['filled_area']}\")\n",
    "                    print(f\"Total Stock Area: {info['total_stock_area']}\\n\")\n",
    "                np.savez(evaluating_history_file_path, \n",
    "                        waste_rate_history=waste_rate_history, \n",
    "                        fitness_history=fitness_history, \n",
    "                        platforms_used_history=platforms_used_history,\n",
    "                        runtime_history=runtime_history)\n",
    "                    \n",
    "        # Render final state\n",
    "        # print(\"step\",steps)\n",
    "        # env.render()\n",
    "        \n",
    "        if not done:\n",
    "            print(f\"Failed to complete order within {max_steps} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_order_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('result/evaluating_history_data.npz')\n",
    "\n",
    "waste_rate_history = data['waste_rate_history']\n",
    "fitness_history = data['fitness_history']\n",
    "platforms_used_history = data['platforms_used_history']\n",
    "runtime_history = data['runtime_history']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "    \n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(waste_rate_history)\n",
    "plt.title('Waste Rate Across Episodes')\n",
    "plt.xlabel('Completed Episode')\n",
    "plt.ylabel('Waste Rate')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(fitness_history)\n",
    "plt.title('Fitness Across Episodes')\n",
    "plt.xlabel('Completed Episode')\n",
    "plt.ylabel('Efficiency')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(platforms_used_history)\n",
    "plt.title('Platforms Used')\n",
    "plt.xlabel('Completed Episode')\n",
    "plt.ylabel('Number of Platforms')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(runtime_history)\n",
    "plt.title('Runtime Across Episodes')\n",
    "plt.xlabel('Completed Episode')\n",
    "plt.ylabel('Runtime')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"result/evaluating_on_50_customer_orders.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
